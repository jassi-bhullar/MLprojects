{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras import backend\n",
    "from keras.layers import Dense, LeakyReLU, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "random_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    # normalize our inputs to be in the range[-1, 1]\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "def get_generator(optimizer):\n",
    "    input_tensor = Input(shape=(random_dim,))\n",
    "    out = Dense(256, kernel_initializer=initializers.RandomNormal(stddev=0.02))(input_tensor)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    \n",
    "    out = Dense(512)(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    \n",
    "    out = Dense(1024)(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    \n",
    "    out = Dense(784, activation='tanh')(out)\n",
    "    generator = Model(input_tensor, out)\n",
    "    generator.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return generator\n",
    "\n",
    "def get_discriminator(optimizer):\n",
    "    input_tensor = Input(shape=(784,))\n",
    "    out = Dense(1024, kernel_initializer=initializers.RandomNormal(stddev=0.02))(input_tensor)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    out = Dropout(0.3)(out)\n",
    "    \n",
    "    out = Dense(512)(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    out = Dropout(0.3)(out)\n",
    "    \n",
    "    out = Dense(256)(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "    out = Dropout(0.3)(out)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "    discriminator = Model(input_tensor, out)\n",
    "    discriminator.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the\n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    out = generator(gan_input)\n",
    "    out = discriminator(out)\n",
    "    \n",
    "    gan = Model(gan_input, out)\n",
    "    gan.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epoch_weights_and_images(epoch, generator, discriminator, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    generator.save_weights('gan_weights/gan_generator_weights_epoch_{}.hdf5'.format(epoch))\n",
    "    discriminator.save_weights('gan_weights/gan_discriminator_weights_epoch_{}.hdf5'.format(epoch))\n",
    "    \n",
    "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_images/mnist_fashion/gan_fashion_epoch_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    x_train, y_train, x_test, y_test = load_fashion_mnist_data()\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    "    \n",
    "    adam = get_optimizer()\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        print('-----------Epoch {}-----------'.format(e))\n",
    "        for batch_num in range(batch_count):\n",
    "            # Generated Images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            # Authentic/Real Images\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            \n",
    "            # Total Input\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            # Output\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "            \n",
    "            # Train the Discriminator\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        if e==1 or e%5==0:\n",
    "            save_epoch_weights_and_images(e, generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Epoch 1-----------\n",
      "-----------Epoch 2-----------\n",
      "-----------Epoch 3-----------\n",
      "-----------Epoch 4-----------\n",
      "-----------Epoch 5-----------\n",
      "-----------Epoch 6-----------\n",
      "-----------Epoch 7-----------\n",
      "-----------Epoch 8-----------\n",
      "-----------Epoch 9-----------\n",
      "-----------Epoch 10-----------\n",
      "-----------Epoch 11-----------\n",
      "-----------Epoch 12-----------\n",
      "-----------Epoch 13-----------\n",
      "-----------Epoch 14-----------\n",
      "-----------Epoch 15-----------\n",
      "-----------Epoch 16-----------\n",
      "-----------Epoch 17-----------\n",
      "-----------Epoch 18-----------\n",
      "-----------Epoch 19-----------\n",
      "-----------Epoch 20-----------\n",
      "-----------Epoch 21-----------\n",
      "-----------Epoch 22-----------\n",
      "-----------Epoch 23-----------\n",
      "-----------Epoch 24-----------\n",
      "-----------Epoch 25-----------\n",
      "-----------Epoch 26-----------\n",
      "-----------Epoch 27-----------\n",
      "-----------Epoch 28-----------\n",
      "-----------Epoch 29-----------\n",
      "-----------Epoch 30-----------\n",
      "-----------Epoch 31-----------\n",
      "-----------Epoch 32-----------\n",
      "-----------Epoch 33-----------\n",
      "-----------Epoch 34-----------\n",
      "-----------Epoch 35-----------\n",
      "-----------Epoch 36-----------\n",
      "-----------Epoch 37-----------\n",
      "-----------Epoch 38-----------\n",
      "-----------Epoch 39-----------\n",
      "-----------Epoch 40-----------\n",
      "-----------Epoch 41-----------\n",
      "-----------Epoch 42-----------\n",
      "-----------Epoch 43-----------\n",
      "-----------Epoch 44-----------\n",
      "-----------Epoch 45-----------\n",
      "-----------Epoch 46-----------\n",
      "-----------Epoch 47-----------\n",
      "-----------Epoch 48-----------\n",
      "-----------Epoch 49-----------\n",
      "-----------Epoch 50-----------\n",
      "-----------Epoch 51-----------\n",
      "-----------Epoch 52-----------\n",
      "-----------Epoch 53-----------\n",
      "-----------Epoch 54-----------\n",
      "-----------Epoch 55-----------\n",
      "-----------Epoch 56-----------\n",
      "-----------Epoch 57-----------\n",
      "-----------Epoch 58-----------\n",
      "-----------Epoch 59-----------\n",
      "-----------Epoch 60-----------\n",
      "-----------Epoch 61-----------\n",
      "-----------Epoch 62-----------\n",
      "-----------Epoch 63-----------\n",
      "-----------Epoch 64-----------\n",
      "-----------Epoch 65-----------\n",
      "-----------Epoch 66-----------\n",
      "-----------Epoch 67-----------\n",
      "-----------Epoch 68-----------\n",
      "-----------Epoch 69-----------\n",
      "-----------Epoch 70-----------\n",
      "-----------Epoch 71-----------\n",
      "-----------Epoch 72-----------\n",
      "-----------Epoch 73-----------\n",
      "-----------Epoch 74-----------\n",
      "-----------Epoch 75-----------\n",
      "-----------Epoch 76-----------\n",
      "-----------Epoch 77-----------\n",
      "-----------Epoch 78-----------\n",
      "-----------Epoch 79-----------\n",
      "-----------Epoch 80-----------\n",
      "-----------Epoch 81-----------\n",
      "-----------Epoch 82-----------\n",
      "-----------Epoch 83-----------\n",
      "-----------Epoch 84-----------\n",
      "-----------Epoch 85-----------\n",
      "-----------Epoch 86-----------\n",
      "-----------Epoch 87-----------\n",
      "-----------Epoch 88-----------\n",
      "-----------Epoch 89-----------\n",
      "-----------Epoch 90-----------\n",
      "-----------Epoch 91-----------\n",
      "-----------Epoch 92-----------\n",
      "-----------Epoch 93-----------\n",
      "-----------Epoch 94-----------\n",
      "-----------Epoch 95-----------\n",
      "-----------Epoch 96-----------\n",
      "-----------Epoch 97-----------\n",
      "-----------Epoch 98-----------\n",
      "-----------Epoch 99-----------\n",
      "-----------Epoch 100-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jassi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Epoch 101-----------\n",
      "-----------Epoch 102-----------\n",
      "-----------Epoch 103-----------\n",
      "-----------Epoch 104-----------\n",
      "-----------Epoch 105-----------\n",
      "-----------Epoch 106-----------\n",
      "-----------Epoch 107-----------\n",
      "-----------Epoch 108-----------\n",
      "-----------Epoch 109-----------\n",
      "-----------Epoch 110-----------\n",
      "-----------Epoch 111-----------\n",
      "-----------Epoch 112-----------\n",
      "-----------Epoch 113-----------\n",
      "-----------Epoch 114-----------\n",
      "-----------Epoch 115-----------\n",
      "-----------Epoch 116-----------\n",
      "-----------Epoch 117-----------\n",
      "-----------Epoch 118-----------\n",
      "-----------Epoch 119-----------\n",
      "-----------Epoch 120-----------\n",
      "-----------Epoch 121-----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1aa1a66bbd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-e022b0bc0235>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0my_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator()\n",
    "generator.load_weights('')\n",
    "\n",
    "discriminator = get_discriminator()\n",
    "discriminator.load_weights('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_gen_dis(generator, discriminator, epochs_offset=0, epochs=1, batch_size=128):\n",
    "    x_train, y_train, x_test, y_test = load_fashion_mnist_data()\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    "    \n",
    "    adam = get_optimizer()\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    "    \n",
    "    for e in range(1+epochs_offset, epochs+1+epochs_offset):\n",
    "        print('-----------Epoch {}-----------'.format(e))\n",
    "        for batch_num in range(batch_count):\n",
    "            # Generated Images\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            # Authentic/Real Images\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            \n",
    "            # Total Input\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            # Output\n",
    "            y_dis = np.zeros(2 * batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "            \n",
    "            # Train the Discriminator\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        if e==1 or e%5==0:\n",
    "            save_epoch_weights_and_images(e+epochs_offset, generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
